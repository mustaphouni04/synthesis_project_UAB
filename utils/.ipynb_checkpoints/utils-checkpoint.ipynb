{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ffe8e-b240-4eed-971d-b78d6cc717cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING NECESSARY LIBRARIES\n",
    "#_________________________________________________________________________________________________________\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import sqlite3\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from string import punctuation\n",
    "from keras.preprocessing import text\n",
    "from urllib.parse import unquote\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "#_________________________________________________________________________________________________________\n",
    "def preprocess_useragent(user_agent):\n",
    "    user_agent = re.sub(r'[/.\\-(),;:]', ' ', user_agent)\n",
    "    tokens = word_tokenize(user_agent)\n",
    "    return tokens\n",
    "    \n",
    "def preprocess_referer(ref):\n",
    "    ref = re.sub(r'[/.-:_=;,]', ' ', ref)\n",
    "    ref = re.sub(r'-', ' ', ref)\n",
    "    tokens = word_tokenize(ref)\n",
    "    return tokens\n",
    "\n",
    "def preprocess_url(url):\n",
    "    url = re.sub(r'[/.-]', ' ', url)\n",
    "    tokens = word_tokenize(url)\n",
    "    return tokens\n",
    "    \n",
    "def embeddings(df: pd.DataFrame, preprocess, feature, emb_dim = 300, epochs = 10, window = 5) -> pd.DataFrame:\n",
    "    # load data\n",
    "    conn = sqlite3.connect('generic.db')\n",
    "    df.to_sql('generic', conn, index=False, if_exists='replace')\n",
    "\n",
    "    # tokenize feature and create corpus\n",
    "    corpus_path = \"corpus\"\n",
    "    gen_tokens = []\n",
    "\n",
    "    chunk_size = 10000\n",
    "    for chunk in pd.read_sql_query(f'SELECT {feature} FROM generic', conn, chunksize=chunk_size):\n",
    "        feats = chunk[feature]\n",
    "        for feat in feats:\n",
    "            decoded_feat = unquote(feat)\n",
    "            tokens = preprocess(decoded_feat)\n",
    "            gen_tokens.append(tokens)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    db_file = 'generic.db'\n",
    "    os.remove(db_file)\n",
    "\n",
    "    # getting the embeddings\n",
    "    EMB_DIM = emb_dim\n",
    "    w2v = Word2Vec(sentences=gen_tokens, vector_size=EMB_DIM, window=window, min_count=5, negative=15,\n",
    "               epochs=epochs, workers=multiprocessing.cpu_count())\n",
    "\n",
    "    word_vectors = w2v.wv\n",
    "\n",
    "    # get the vocabulary\n",
    "    vocab = list(word_vectors.key_to_index.keys())\n",
    "\n",
    "    # get the word vectors\n",
    "    word_vecs = [word_vectors[word] for word in vocab]\n",
    "\n",
    "    # convert word_vecs to a numpy array\n",
    "    word_vecs_np = np.array(word_vecs)\n",
    "\n",
    "    # create a DataFrame for the embeddings\n",
    "    embeddings_df = pd.DataFrame(word_vecs_np, index=vocab)\n",
    "\n",
    "    # merge embeddings with the original DataFrame\n",
    "    df_with_embeddings = pd.merge(df, embeddings_df, left_on=feature, right_index=True, how='left')\n",
    "\n",
    "    return df_with_embeddings\n",
    "\n",
    "# PREPROCESSING FUNCTIONS\n",
    "#_________________________________________________________________________________________________________\n",
    "def preprocessing_TS(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    TS: Cyclic encoding of timestamp.\n",
    "    \"\"\"\n",
    "    # convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%b/%Y:%H:%M:%S %z')\n",
    "\n",
    "    # extract hour, minute, and second components\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['minute'] = df['timestamp'].dt.minute\n",
    "    df['second'] = df['timestamp'].dt.second\n",
    "\n",
    "    # encode hour, minute, and second with sine and cosine functions\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['minute_sin'] = np.sin(2 * np.pi * df['minute'] / 60)\n",
    "    df['minute_cos'] = np.cos(2 * np.pi * df['minute'] / 60)\n",
    "    df['second_sin'] = np.sin(2 * np.pi * df['second'] / 60)\n",
    "    df['second_cos'] = np.cos(2 * np.pi * df['second'] / 60)\n",
    "\n",
    "    # drop original timestamp and hour, minute, and second columns\n",
    "    df.drop(['timestamp', 'hour', 'minute', 'second'], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocessing_SC(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    SC: One-Hot encoding status_code.\n",
    "    \"\"\"\n",
    "    one_hot_encoded = pd.get_dummies(df['status_code'], prefix='status_code')\n",
    "    df_encoded = pd.concat([df, one_hot_encoded], axis=1)\n",
    "    df_encoded.drop('status_code', axis=1, inplace=True)\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "def preprocessing_RM(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    RM: One-Hot encoding request_method.\n",
    "    \"\"\"\n",
    "    one_hot_encoded = pd.get_dummies(df['request_method'], prefix='request_method')\n",
    "    df_encoded = pd.concat([df, one_hot_encoded], axis=1)\n",
    "    df_encoded.drop('request_method', axis=1, inplace=True)\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "def preprocessing_RH(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    RH: Normalizing remote_host and splitting into octets.\n",
    "    \"\"\"\n",
    "    # split the IP address into four separate columns\n",
    "    df[['octet1', 'octet2', 'octet3', 'octet4']] = df['remote_host'].str.split('.', expand=True)\n",
    "    df[['octet1', 'octet2', 'octet3', 'octet4']] = df[['octet1', 'octet2', 'octet3', 'octet4']].apply(pd.to_numeric)\n",
    "    df[['octet1', 'octet2', 'octet3', 'octet4']] /= 255\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocessing_R(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    R: Generating embeddings for referer.\n",
    "    \"\"\"\n",
    "    # drop all - \n",
    "    df = df[df['referer'] != '-']\n",
    "    \n",
    "    df_with_embeddings = embeddings(df, preprocess_referer, feature = 'referer', emb_dim = 300, epochs = 10, window = 5)  \n",
    "\n",
    "    return df_with_embeddings\n",
    "\n",
    "def preprocessing_RU(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    RU: Generating embeddings for requested_url.\n",
    "    \"\"\"   \n",
    "    df_with_embeddings = embeddings(df, preprocess_url, feature = 'requested_url', emb_dim = 300, epochs = 10, window = 5)  \n",
    "\n",
    "    return df_with_embeddings\n",
    "\n",
    "def preprocessing_BS(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    BS: Normalizing bytes_sent.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def preprocessing_UA(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    UA: Generating embeddings for user_agent.\n",
    "    \"\"\"\n",
    "    df_with_embeddings = embeddings(df, preprocess_useragent, feature = 'user_agent', emb_dim = 300, epochs = 10, window = 5)  \n",
    "\n",
    "    return df_with_embeddings\n",
    "\n",
    "def preprocessing_CLEAN(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    CLEAN: Drop all unnecessary columns.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tf4)",
   "language": "python",
   "name": "tensorflowv4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
